{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ad0b2a",
   "metadata": {},
   "source": [
    "# UDF(User Defined Function)\n",
    "- 사용자 정의 함수\n",
    "- 데이터프레임에도 사용이 가능, SQL에서도 사용이 가능!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4fce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/12 08:07:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"udf\").getOrCreate()\n",
    "\n",
    "transactions = [\n",
    "    ('찹쌀탕수육+짜장2', '2021-11-07 13:20:00', 22000, 'KRW'),\n",
    "    ('등심탕수육+크립새우+짜장면', '2021-10-24 11:19:00', 21500, 'KRW'), \n",
    "    ('월남 쌈 2인 세트', '2021-07-25 11:12:40', 42000, 'KRW'), \n",
    "    ('콩국수+열무비빔국수', '2021-07-10 08:20:00', 21250, 'KRW'), \n",
    "    ('장어소금+고추장구이', '2021-07-01 05:36:00', 68700, 'KRW'), \n",
    "    ('족발', '2020-08-19 19:04:00', 32000, 'KRW'),  \n",
    "]\n",
    "\n",
    "schema = [\"name\", \"datetime\", \"price\", \"currency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ca6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=transactions, schema=schema)\n",
    "df.createOrReplaceTempView(\"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b2b6db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------------------+-----+--------+\n",
      "|                      name|           datetime|price|currency|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "|          찹쌀탕수육+짜장2|2021-11-07 13:20:00|22000|     KRW|\n",
      "|등심탕수육+크립새우+짜장면|2021-10-24 11:19:00|21500|     KRW|\n",
      "|          월남 쌈 2인 세트|2021-07-25 11:12:40|42000|     KRW|\n",
      "|       콩국수+열무비빔국수|2021-07-10 08:20:00|21250|     KRW|\n",
      "|       장어소금+고추장구이|2021-07-01 05:36:00|68700|     KRW|\n",
      "|                      족발|2020-08-19 19:04:00|32000|     KRW|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from transactions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56170b",
   "metadata": {},
   "source": [
    "- 분산 병렬 처리 환경에서 사용할 수 있는 함수를 만들기(Worker에서 작동하는 함수)\n",
    "- 리턴 타입을 따로 지정하지 않으면 기본적으로 String을 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff02fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# 이 함수는 마스터 노드에서 사용함수.. 따라서 Worker에서 작동하지 않는다.\n",
    "def squared(n):\n",
    "    return n * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5aa9943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(n)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register(\"Worker에서 사용할 함수의 이름\", 마스터에 정의된 함수 이름, 리턴 타입)\n",
    "\n",
    "spark.udf.register(\"squared\", squared, LongType()) # Worker에서 함수가 작동될 수 있도록 udf로 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "080137f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|price|squared(price)|\n",
      "+-----+--------------+\n",
      "|22000|     484000000|\n",
      "|21500|     462250000|\n",
      "|42000|    1764000000|\n",
      "|21250|     451562500|\n",
      "|68700|    4719690000|\n",
      "|32000|    1024000000|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT price, squared(price)\n",
    "    FROM transactions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5685eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_number(n):\n",
    "    units = [\"\", \"십\", \"백\", \"천\", \"만\"]\n",
    "    nums = '일이삼사오육칠팔구'\n",
    "    result = []\n",
    "    i = 0\n",
    "    while n > 0:\n",
    "        n, r = divmod(n, 10)\n",
    "        if r > 0:\n",
    "            result.append(nums[r-1]+units[i])\n",
    "        i += 1\n",
    "    return \"\".join(reversed(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65b78c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'삼만오천'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_number(35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85471f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.read_number(n)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"read_number\", read_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c481ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|price|read_number(price)|\n",
      "+-----+------------------+\n",
      "|22000|          이만이천|\n",
      "|21500|      이만일천오백|\n",
      "|42000|          사만이천|\n",
      "|21250|  이만일천이백오십|\n",
      "|68700|      육만팔천칠백|\n",
      "|32000|          삼만이천|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT price, read_number(price)\n",
    "    FROM transactions\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0467d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_weekday(date)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요일을 구해내는 함수 만들기\n",
    "def get_weekday(date):\n",
    "    import calendar\n",
    "    return calendar.day_name[date.weekday()]\n",
    "\n",
    "spark.udf.register(\"get_weekday\", get_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7d79166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|           datetime|day_of_week|\n",
      "+-------------------+-----------+\n",
      "|2021-11-07 13:20:00|     Sunday|\n",
      "|2021-10-24 11:19:00|     Sunday|\n",
      "|2021-07-25 11:12:40|     Sunday|\n",
      "|2021-07-10 08:20:00|   Saturday|\n",
      "|2021-07-01 05:36:00|   Thursday|\n",
      "|2020-08-19 19:04:00|  Wednesday|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "SELECT datetime, get_weekday(TO_DATE(datetime)) as day_of_week\n",
    "FROM transactions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47af0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/ubuntu/working/spark-examples/data/titanic_train.csv\"\n",
    "titanic_sdf = spark.read.csv(filepath, inferSchema=True, header=True)\n",
    "\n",
    "titanic_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0205603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "avg_age = titanic_sdf.select(F.avg(F.col('Age')))\n",
    "avg_age_row = avg_age.head()\n",
    "avg_age_value = avg_age.head()[0]\n",
    "\n",
    "# Spark DataFrame의 fillna()에 인자로 Dict를 입력하여 여러개의 컬럼들에 대해서 결측치 값을 입력할 수 있게 만들어줌. \n",
    "titanic_sdf_filled = titanic_sdf.fillna({'Age': avg_age_value, \n",
    "                                         'Cabin': 'C000',\n",
    "                                         'Embarked': 'S'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a380109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(age):\n",
    "    cat = ''\n",
    "    \n",
    "    if age <= 5: cat = 'Baby'\n",
    "    elif age <= 12: cat = 'Child'\n",
    "    elif age <= 18: cat = 'Teenager'\n",
    "    elif age <= 25: cat = 'Student'\n",
    "    elif age <= 35: cat = 'Young Adult'\n",
    "    elif age <= 60: cat = 'Adult'\n",
    "    else : cat = 'Elderly'\n",
    "    \n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab21f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임 API에서 UDF 사용하기\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "udf_get_category = udf(lambda x : get_category(x), StringType())\n",
    "udf_get_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc02de70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+-----------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|              Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|AgeCategory|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+-----------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|             22.0|    1|    0|       A/5 21171|   7.25| C000|       S|    Student|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|             38.0|    1|    0|        PC 17599|71.2833|  C85|       C|      Adult|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|             26.0|    0|    0|STON/O2. 3101282|  7.925| C000|       S|Young Adult|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|             35.0|    1|    0|          113803|   53.1| C123|       S|Young Adult|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|             35.0|    0|    0|          373450|   8.05| C000|       S|Young Adult|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|29.69911764705882|    0|    0|          330877| 8.4583| C000|       Q|Young Adult|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|             54.0|    0|    0|           17463|51.8625|  E46|       S|      Adult|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|              2.0|    3|    1|          349909| 21.075| C000|       S|       Baby|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|             27.0|    0|    2|          347742|11.1333| C000|       S|Young Adult|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|             14.0|    1|    0|          237736|30.0708| C000|       C|   Teenager|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|              4.0|    1|    1|         PP 9549|   16.7|   G6|       S|       Baby|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|             58.0|    0|    0|          113783|  26.55| C103|       S|      Adult|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|             20.0|    0|    0|       A/5. 2151|   8.05| C000|       S|    Student|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|             39.0|    1|    5|          347082| 31.275| C000|       S|      Adult|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|             14.0|    0|    0|          350406| 7.8542| C000|       S|   Teenager|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|             55.0|    0|    0|          248706|   16.0| C000|       S|      Adult|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|              2.0|    4|    1|          382652| 29.125| C000|       Q|       Baby|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|29.69911764705882|    0|    0|          244373|   13.0| C000|       S|Young Adult|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|             31.0|    1|    0|          345763|   18.0| C000|       S|Young Adult|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|29.69911764705882|    0|    0|            2649|  7.225| C000|       C|Young Adult|\n",
      "+-----------+--------+------+--------------------+------+-----------------+-----+-----+----------------+-------+-----+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_sdf_filled_01 = titanic_sdf_filled.withColumn(\"AgeCategory\", udf_get_category(col(\"Age\")))\n",
    "titanic_sdf_filled_01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a10ba90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

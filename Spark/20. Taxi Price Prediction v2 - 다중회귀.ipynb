{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6341b774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/14 01:01:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()\n",
    "\n",
    "directory=\"/home/ubuntu/working/spark-examples/data\"\n",
    "trip_files=\"trips/*\"\n",
    "\n",
    "trips_df = spark.read.csv(f\"file://{directory}/{trip_files}\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623c2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac2791a",
   "metadata": {},
   "source": [
    "달라진 점\n",
    " - 첫 번째 모델은 너무 데이터의 종류가 적어서 예측 성능이 좋지 못했기 때문에 데이터의 종류를 좀 늘려볼 예정!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d68fb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') AS day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 5\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\"\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95c759a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "|              0|               138|                265|         16.5|          0|     Monday|       70.07|\n",
      "|              1|                68|                264|         1.13|          0|     Monday|       11.16|\n",
      "|              1|               239|                262|         2.68|          0|     Monday|       18.59|\n",
      "|              1|               186|                 91|         12.4|          0|     Monday|        43.8|\n",
      "|              2|               132|                265|          9.7|          0|     Monday|        32.3|\n",
      "|              1|               138|                141|          9.3|          0|     Monday|       43.67|\n",
      "|              1|               138|                 50|         9.58|          0|     Monday|        46.1|\n",
      "|              1|               132|                123|         16.2|          0|     Monday|        45.3|\n",
      "|              1|               140|                  7|         3.58|          0|     Monday|        19.3|\n",
      "|              1|               239|                238|         0.91|          0|     Monday|        14.8|\n",
      "|              2|               116|                 41|         2.57|          0|     Monday|        12.8|\n",
      "|              1|                74|                 41|          0.4|          0|     Monday|         5.3|\n",
      "|              1|               239|                144|         3.26|          0|     Monday|        17.3|\n",
      "|              1|               132|                 91|        13.41|          0|     Monday|       47.25|\n",
      "|              2|               132|                230|         18.3|          0|     Monday|       61.42|\n",
      "|              1|               229|                 48|         1.53|          0|     Monday|       14.16|\n",
      "|              1|                48|                 68|          2.0|          0|     Monday|        11.8|\n",
      "|              2|               132|                255|         16.6|          0|     Monday|       54.96|\n",
      "|              1|               132|                145|         15.5|          0|     Monday|       56.25|\n",
      "|              2|                79|                164|          1.3|          0|     Monday|        16.8|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31c1c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d740eb",
   "metadata": {},
   "source": [
    "지금처럼 데이터의 양이 너무나 많고, 그 데이터에 대한 전처리를 수행 했음. 그래서 시간이 굉장히 많이 걸렸다고 가정\n",
    "- 사용할 데이터 마트의 구축이 완료된 상황이라고 가정\n",
    "- 추후에 다시 이 데이터를 다시 활용한다면 다시 처음부터 전처리하는데 시간이 많이 걸린다..\n",
    "- 이렇게 전처리가 된 데이터를 파일이나 데이터베이스에 저장 해놓고, 나중에 다시 불러오는게 훨씬 시간적으로 이득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59162322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 파케이(parquet) 형식으로 데이터 마트를 저장\n",
    "data_dir = \"/home/ubuntu/working/spark-examples/data/ml-data\"\n",
    "\n",
    "# Spark Dataframe의 write를 이용해 데이터를 파일 또는 데이터베이스에 저장이 가능\n",
    "train_df.write.format(\"parquet\").save(f\"{data_dir}/train/\")\n",
    "test_df.write.format(\"parquet\").save(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14043a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분산 저장 되어서 파티션으로 분리된 파일을 불러오기\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()\n",
    "data_dir = \"/home/ubuntu/working/spark-examples/data/ml-data\"\n",
    "\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2154e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ec96c",
   "metadata": {},
   "source": [
    "# 파이프라인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6636e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline에 넣을 과정(stage)을 하나씩 넣어 놓을 리스트\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d486fe",
   "metadata": {},
   "source": [
    "## 1. OneHotEncoding Stage\n",
    "- `pickup_location_id`\n",
    "- `dropoff_location_id`\n",
    "- `day_of_week`\n",
    "\n",
    "`pickup_location_id`, `dropoff_location_id`는 숫자 형식의 데이터!\n",
    "- 숫자 형태의 데이터는 OneHotEncoding이 안된다.\n",
    "- 숫자 형식의 카테고리 데이터를 임시로 문자열로 처리하기 위해 `StringIndexer` 트랜스포머를 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c7eb0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_6a5e2395d543,\n",
       " OneHotEncoder_831e5028990e,\n",
       " StringIndexer_cc289a352393,\n",
       " OneHotEncoder_e69185437cd6,\n",
       " StringIndexer_b1054a91b76a,\n",
       " OneHotEncoder_964f74c9b6f1]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# OneHotEncoding을 수행할 컬럼을 지정\n",
    "cat_features = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "for c in cat_features:\n",
    "    # 1. 데이터를 문자열 형식으로 바꿔준다. setHandleInvalid : Null값 같은 데이터를 어떻게 처리 할건지\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol=c+\"_idx\").setHandleInvalid(\"keep\")\n",
    "    \n",
    "    # 2. One Hot Encoding 수행\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c+\"_onehot\"])\n",
    "    \n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69797341",
   "metadata": {},
   "source": [
    "## 2. StandardScaler & VectorAssembler Stage\n",
    "- `passenger_count`\n",
    "- `trip_distance`\n",
    "- `pickup_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68531019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_6a5e2395d543,\n",
       " OneHotEncoder_831e5028990e,\n",
       " StringIndexer_cc289a352393,\n",
       " OneHotEncoder_e69185437cd6,\n",
       " StringIndexer_b1054a91b76a,\n",
       " OneHotEncoder_964f74c9b6f1,\n",
       " VectorAssembler_eba5f07be7e9,\n",
       " StandardScaler_ce644ce26efa,\n",
       " VectorAssembler_bd6747be5d25,\n",
       " StandardScaler_520955cb966b,\n",
       " VectorAssembler_fd46d0e2b56a,\n",
       " StandardScaler_0b84442dc758]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_features = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_features:\n",
    "    \n",
    "    # 각각의 컬럼의 데이터가 벡터화. ex) 1.5 -> [1.5]\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol=n+\"_vector\")\n",
    "    \n",
    "    # StandardScaling 수행\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol=n+\"_scaled\")\n",
    "    \n",
    "    stages += [num_assembler, num_scaler]\n",
    "\n",
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e71e356",
   "metadata": {},
   "source": [
    "머신러닝을 위한 Preprocessing된 결과물 벡터를 하나로 합쳐야 훈련 가능한 데이터가 된다. - `VectorAssembler`를 사용해서 합친다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d239f00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickup_location_id_onehot',\n",
       " 'dropoff_location_id_onehot',\n",
       " 'day_of_week_onehot',\n",
       " 'passenger_count_scaled',\n",
       " 'trip_distance_scaled',\n",
       " 'pickup_time_scaled']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assemble 할 데이터는?\n",
    "# OntHotEncoding이 되어 있거나, Scaled된 데이터를 합쳐줘야 한다.\n",
    "\n",
    "assembler_inputs = [c + \"_onehot\" for c in cat_features] + [n + \"_scaled\" for n in num_features]\n",
    "assembler_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85fb6625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_6a5e2395d543,\n",
       " OneHotEncoder_831e5028990e,\n",
       " StringIndexer_cc289a352393,\n",
       " OneHotEncoder_e69185437cd6,\n",
       " StringIndexer_b1054a91b76a,\n",
       " OneHotEncoder_964f74c9b6f1,\n",
       " VectorAssembler_eba5f07be7e9,\n",
       " StandardScaler_ce644ce26efa,\n",
       " VectorAssembler_bd6747be5d25,\n",
       " StandardScaler_520955cb966b,\n",
       " VectorAssembler_fd46d0e2b56a,\n",
       " StandardScaler_0b84442dc758,\n",
       " VectorAssembler_9496fbcda261]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages.append(total_assembler)\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a571ee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_02c52eb987fc"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이프라인 등록\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66d0b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineModel_cf93762ed89e"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "fitted_transformer = pipeline.fit(train_df)\n",
    "fitted_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f3eb2",
   "metadata": {},
   "source": [
    "transformer 파이프라인을 이용해 tran_df 데이터 변환 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c55998a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_location_id: integer (nullable = true)\n",
      " |-- dropoff_location_id: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vector: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vector: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vector: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_train_df = fitted_transformer.transform(train_df)\n",
    "vec_train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b34cf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:>                 (0 + 2) / 2][Stage 22:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(534,[62,312,530,...|\n",
      "|(534,[62,312,528,...|\n",
      "|(534,[62,273,528,...|\n",
      "|(534,[62,281,525,...|\n",
      "|(534,[62,309,524,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"refresh progress\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.status.AppStatusStore.activeStages(AppStatusStore.scala:114)\n",
      "\tat org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:64)\n",
      "\tat org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:52)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "Exception in thread \"RemoteBlock-temp-file-clean-thread\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/06/14 02:31:43 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/06/14 02:31:43 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "# vec_train_df.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3e515",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4595839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver='normal',\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"features\",\n",
    "    regParam=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b184f9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/14 02:34:05 WARN Instrumentation: [0e1a4630] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/14 02:35:27 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/06/14 02:35:27 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/06/14 02:35:29 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/06/14 02:35:30 WARN Instrumentation: [0e1a4630] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n",
      "23/06/14 02:35:30 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/06/14 02:35:30 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = lr.fit(vec_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8274b3f2",
   "metadata": {},
   "source": [
    "# 테스트 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85bd2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df 변환\n",
    "vec_test_df = fitted_transformer.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "548154a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|passenger_count|pickup_location_id|dropoff_location_id|trip_distance|pickup_time|day_of_week|total_amount|pickup_location_id_idx|pickup_location_id_onehot|dropoff_location_id_idx|dropoff_location_id_onehot|day_of_week_idx|day_of_week_onehot|passenger_count_vector|passenger_count_scaled|trip_distance_vector|trip_distance_scaled|pickup_time_vector|  pickup_time_scaled|            features|        prediction|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "|              0|                 4|                107|          1.0|         12|   Thursday|       12.25|                  62.0|         (263,[62],[1.0])|                   17.0|          (261,[17],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [1.0]|[0.26068182750920...|            [12.0]| [2.343538284265401]|(534,[62,280,525,...|13.251716595308377|\n",
      "|              0|                 4|                170|          2.2|         21|   Saturday|        14.3|                  62.0|         (263,[62],[1.0])|                    4.0|           (261,[4],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|               [2.2]|[0.5735000205202505]|            [21.0]|[4.1011919974644515]|(534,[62,267,528,...|15.914445362429937|\n",
      "|              0|                 7|                  7|          1.0|         11|     Monday|         7.3|                  63.0|         (263,[63],[1.0])|                   57.0|          (261,[57],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [1.0]|[0.26068182750920...|            [11.0]| [2.148243427243284]|(534,[63,320,529,...|10.278181010473554|\n",
      "|              0|                 7|                 79|          5.2|         14|     Friday|        25.3|                  63.0|         (263,[63],[1.0])|                   18.0|          (261,[18],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [5.2]| [1.355545503047865]|            [14.0]|[2.7341279983096345]|(534,[63,281,524,...| 22.23538812288367|\n",
      "|              0|                 7|                129|          1.9|          2|     Monday|        11.3|                  63.0|         (263,[63],[1.0])|                   71.0|          (261,[71],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [1.9]|[0.49529547226748...|             [2.0]|[0.3905897140442335]|(534,[63,334,529,...|11.643369460731456|\n",
      "|              0|                 7|                138|          4.5|         15|   Thursday|        17.8|                  63.0|         (263,[63],[1.0])|                   48.0|          (261,[48],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [4.5]|[1.1730682237914214]|            [15.0]| [2.929422855331751]|(534,[63,311,525,...|28.273347378101008|\n",
      "|              0|                 7|                193|          0.4|         16|   Thursday|         5.8|                  63.0|         (263,[63],[1.0])|                  108.0|         (261,[108],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|               [0.4]|[0.10427273100368...|            [16.0]| [3.124717712353868]|(534,[63,371,525,...| 7.408246384979956|\n",
      "|              0|                 9|                  9|         15.4|         20|   Thursday|        65.3|                 232.0|        (263,[232],[1.0])|                  219.0|         (261,[219],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|              [15.4]| [4.014500143641754]|            [20.0]|[3.9058971404423346]|(534,[232,482,525...| 45.73283006976534|\n",
      "|              0|                10|                 41|         15.6|         16|   Thursday|       63.85|                  84.0|         (263,[84],[1.0])|                   37.0|          (261,[37],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|              [15.6]| [4.066636509143594]|            [16.0]| [3.124717712353868]|(534,[84,300,525,...| 68.54619282108459|\n",
      "|              0|                10|                113|         18.9|         10|   Thursday|        63.3|                  84.0|         (263,[84],[1.0])|                   32.0|          (261,[32],[1.0])|            1.0|     (7,[1],[1.0])|                 [0.0]|                 [0.0]|              [18.9]|  [4.92688653992397]|            [10.0]|[1.9529485702211673]|(534,[84,295,525,...| 77.10267278956672|\n",
      "|              0|                10|                233|         13.6|         16|   Saturday|       61.85|                  84.0|         (263,[84],[1.0])|                   27.0|          (261,[27],[1.0])|            4.0|     (7,[4],[1.0])|                 [0.0]|                 [0.0]|              [13.6]|[3.5452728541251846]|            [16.0]| [3.124717712353868]|(534,[84,290,528,...|62.643116917470884|\n",
      "|              0|                10|                246|         15.2|         17|  Wednesday|        76.3|                  84.0|         (263,[84],[1.0])|                   24.0|          (261,[24],[1.0])|            2.0|     (7,[2],[1.0])|                 [0.0]|                 [0.0]|              [15.2]|[3.9623637781399124]|            [17.0]|[3.3200125693759848]|(534,[84,287,526,...| 68.12184233120922|\n",
      "|              0|                12|                  4|          3.3|         11|     Friday|        15.8|                  71.0|         (263,[71],[1.0])|                   49.0|          (261,[49],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [3.3]|[0.8602500307803757]|            [11.0]| [2.148243427243284]|(534,[71,312,524,...| 19.71758435607552|\n",
      "|              0|                12|                 79|          3.7|         20|     Monday|        16.8|                  71.0|         (263,[71],[1.0])|                   18.0|          (261,[18],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [3.7]|[0.9645227617840577]|            [20.0]|[3.9058971404423346]|(534,[71,281,529,...|20.751286369235252|\n",
      "|              0|                12|                100|          6.3|         17|     Monday|        24.3|                  71.0|         (263,[71],[1.0])|                   30.0|          (261,[30],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [6.3]|  [1.64229551330799]|            [17.0]|[3.3200125693759848]|(534,[71,293,529,...|26.952429587498887|\n",
      "|              0|                12|                233|          5.2|         13|     Monday|       23.15|                  71.0|         (263,[71],[1.0])|                   27.0|          (261,[27],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [5.2]| [1.355545503047865]|            [13.0]|[2.5388331412875176]|(534,[71,290,529,...|23.690706368190263|\n",
      "|              0|                13|                 13|          0.1|         22|     Monday|         6.8|                  47.0|         (263,[47],[1.0])|                   44.0|          (261,[44],[1.0])|            5.0|     (7,[5],[1.0])|                 [0.0]|                 [0.0]|               [0.1]|[0.02606818275092...|            [22.0]| [4.296486854486568]|(534,[47,307,529,...|12.852588937953684|\n",
      "|              0|                13|                 13|          0.5|         15|     Friday|        9.45|                  47.0|         (263,[47],[1.0])|                   44.0|          (261,[44],[1.0])|            0.0|     (7,[0],[1.0])|                 [0.0]|                 [0.0]|               [0.5]|[0.13034091375460...|            [15.0]| [2.929422855331751]|(534,[47,307,524,...|14.104161988728377|\n",
      "|              0|                13|                 68|          2.6|         18|    Tuesday|        15.3|                  47.0|         (263,[47],[1.0])|                   14.0|          (261,[14],[1.0])|            3.0|     (7,[3],[1.0])|                 [0.0]|                 [0.0]|               [2.6]|[0.6777727515239325]|            [18.0]|[3.5153074263981012]|(534,[47,277,527,...| 18.02904228422226|\n",
      "|              0|                13|                 68|          3.4|         21|     Sunday|        17.8|                  47.0|         (263,[47],[1.0])|                   14.0|          (261,[14],[1.0])|            6.0|     (7,[6],[1.0])|                 [0.0]|                 [0.0]|               [3.4]|[0.8863182135312961]|            [21.0]|[4.1011919974644515]|(534,[47,277,530,...|19.350870180727114|\n",
      "+---------------+------------------+-------------------+-------------+-----------+-----------+------------+----------------------+-------------------------+-----------------------+--------------------------+---------------+------------------+----------------------+----------------------+--------------------+--------------------+------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vec_test_df로 예측\n",
    "predictions = model.transform(vec_test_df)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a218712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[passenger_count: int, pickup_location_id: int, dropoff_location_id: int, trip_distance: double, pickup_time: int, day_of_week: string, total_amount: double, pickup_location_id_idx: double, pickup_location_id_onehot: vector, dropoff_location_id_idx: double, dropoff_location_id_onehot: vector, day_of_week_idx: double, day_of_week_onehot: vector, passenger_count_vector: vector, passenger_count_scaled: vector, trip_distance_vector: vector, trip_distance_scaled: vector, pickup_time_vector: vector, pickup_time_scaled: vector, features: vector, prediction: double]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측한 결과를 따로 확인 할 때는 조회만 일어난다. predictions 데이터도 cache() 처리하는 것이 좋음!\n",
    "predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21caaa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/14 02:38:12 WARN MemoryStore: Not enough space to cache rdd_83_0 in memory! (computed 440.4 MiB so far)\n",
      "23/06/14 02:38:12 WARN BlockManager: Persisting block rdd_83_0 to disk instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+------------------+\n",
      "|trip_distance|day_of_week|total_amount|        prediction|\n",
      "+-------------+-----------+------------+------------------+\n",
      "|          1.0|   Thursday|       12.25|13.251716595308377|\n",
      "|          2.2|   Saturday|        14.3|15.914445362429937|\n",
      "|          1.0|     Monday|         7.3|10.278181010473554|\n",
      "|          5.2|     Friday|        25.3| 22.23538812288367|\n",
      "|          1.9|     Monday|        11.3|11.643369460731456|\n",
      "|          4.5|   Thursday|        17.8|28.273347378101008|\n",
      "|          0.4|   Thursday|         5.8| 7.408246384979956|\n",
      "|         15.4|   Thursday|        65.3| 45.73283006976534|\n",
      "|         15.6|   Thursday|       63.85| 68.54619282108459|\n",
      "|         18.9|   Thursday|        63.3| 77.10267278956672|\n",
      "|         13.6|   Saturday|       61.85|62.643116917470884|\n",
      "|         15.2|  Wednesday|        76.3| 68.12184233120922|\n",
      "|          3.3|     Friday|        15.8| 19.71758435607552|\n",
      "|          3.7|     Monday|        16.8|20.751286369235252|\n",
      "|          6.3|     Monday|        24.3|26.952429587498887|\n",
      "|          5.2|     Monday|       23.15|23.690706368190263|\n",
      "|          0.1|     Monday|         6.8|12.852588937953684|\n",
      "|          0.5|     Friday|        9.45|14.104161988728377|\n",
      "|          2.6|    Tuesday|        15.3| 18.02904228422226|\n",
      "|          3.4|     Sunday|        17.8|19.350870180727114|\n",
      "+-------------+-----------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/14 02:38:29 WARN MemoryStore: Not enough space to cache rdd_83_0 in memory! (computed 440.4 MiB so far)\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.select(\"trip_distance\", \"day_of_week\", \"total_amount\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc05fb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.89274566627785"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1eb055b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7957936214535244"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9be2c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

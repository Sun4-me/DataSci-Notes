{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6662d6",
   "metadata": {},
   "source": [
    "# 카탈리스트\n",
    "- 쿼리에 대한 논리적인 실행 계획을 수립\n",
    "- 쿼리 분석 -> 논리적 계획 수립 -> 물리적 계획 수립"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efaf79e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/07 04:06:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"trip_count_sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096ee7c",
   "metadata": {},
   "source": [
    "CSV를 불러와서 Spark DF으로 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39503cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/ubuntu/working/spark-examples/data/titanic_train.csv\"\n",
    "titanic_sdf = spark.read.csv(filepath,\n",
    "                             inferSchema=True, # 데이터의 타입을 스파크가 자동으로 인식\n",
    "                             header=True, # 첫 줄을 불러 올지 말지 결정\n",
    "                            )\n",
    "\n",
    "titanic_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda2bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sdf.createOrReplaceTempView(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a8d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select *\n",
    "from titanic\n",
    "limit 5\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2139a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+\n",
      "|Embarked|Pclass|male_cnt|\n",
      "+--------+------+--------+\n",
      "|       C|     1|      42|\n",
      "|       C|     2|      10|\n",
      "|       C|     3|      43|\n",
      "|       Q|     1|       1|\n",
      "|       Q|     2|       1|\n",
      "|       Q|     3|      39|\n",
      "|       S|     1|      79|\n",
      "|       S|     2|      97|\n",
      "|       S|     3|     265|\n",
      "+--------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 남자 중 Embarked(탑승지) 별 Pclass(좌석 등급) 마다 몇 명이 탔는지\n",
    "query = \"\"\"\n",
    "select Embarked, Pclass, count(*) as male_cnt\n",
    "from titanic\n",
    "where Sex='male'\n",
    "group by Embarked, Pclass\n",
    "order by Embarked, Pclass\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04df8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Embarked ASC NULLS FIRST, 'Pclass ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['Embarked, 'Pclass], ['Embarked, 'Pclass, 'count(1) AS male_cnt#220]\n",
      "   +- 'Filter ('Sex = male)\n",
      "      +- 'UnresolvedRelation [titanic], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Embarked: string, Pclass: int, male_cnt: bigint\n",
      "Sort [Embarked#27 ASC NULLS FIRST, Pclass#18 ASC NULLS FIRST], true\n",
      "+- Aggregate [Embarked#27, Pclass#18], [Embarked#27, Pclass#18, count(1) AS male_cnt#220L]\n",
      "   +- Filter (Sex#20 = male)\n",
      "      +- SubqueryAlias titanic\n",
      "         +- View (`titanic`, [PassengerId#16,Survived#17,Pclass#18,Name#19,Sex#20,Age#21,SibSp#22,Parch#23,Ticket#24,Fare#25,Cabin#26,Embarked#27])\n",
      "            +- Relation [PassengerId#16,Survived#17,Pclass#18,Name#19,Sex#20,Age#21,SibSp#22,Parch#23,Ticket#24,Fare#25,Cabin#26,Embarked#27] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Embarked#27 ASC NULLS FIRST, Pclass#18 ASC NULLS FIRST], true\n",
      "+- Aggregate [Embarked#27, Pclass#18], [Embarked#27, Pclass#18, count(1) AS male_cnt#220L]\n",
      "   +- Project [Pclass#18, Embarked#27]\n",
      "      +- Filter (isnotnull(Sex#20) AND (Sex#20 = male))\n",
      "         +- Relation [PassengerId#16,Survived#17,Pclass#18,Name#19,Sex#20,Age#21,SibSp#22,Parch#23,Ticket#24,Fare#25,Cabin#26,Embarked#27] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Embarked#27 ASC NULLS FIRST, Pclass#18 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Embarked#27 ASC NULLS FIRST, Pclass#18 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=218]\n",
      "      +- HashAggregate(keys=[Embarked#27, Pclass#18], functions=[count(1)], output=[Embarked#27, Pclass#18, male_cnt#220L])\n",
      "         +- Exchange hashpartitioning(Embarked#27, Pclass#18, 200), ENSURE_REQUIREMENTS, [plan_id=215]\n",
      "            +- HashAggregate(keys=[Embarked#27, Pclass#18], functions=[partial_count(1)], output=[Embarked#27, Pclass#18, count#226L])\n",
      "               +- Project [Pclass#18, Embarked#27]\n",
      "                  +- Filter (isnotnull(Sex#20) AND (Sex#20 = male))\n",
      "                     +- FileScan csv [Pclass#18,Sex#20,Embarked#27] Batched: false, DataFilters: [isnotnull(Sex#20), (Sex#20 = male)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/working/spark-examples/data/titanic_train.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Sex), EqualTo(Sex,male)], ReadSchema: struct<Pclass:int,Sex:string,Embarked:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0261c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69cc845",
   "metadata": {},
   "source": [
    "## 텍시 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d76a035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/07 04:34:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"taxi\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d27a1384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   null|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#csv 불러와서 Spark DF으로 만들어보기\n",
    "filepath = \"/home/ubuntu/working/spark-examples/data/fhvhv_tripdata_2020-03.csv\"\n",
    "\n",
    "taxi_sdf = spark.read.csv(filepath,\n",
    "                            inferSchema=True, #데이터의 타입을 스파크가 자동으로 인식\n",
    "                             header=True, #헤더의 첫줄을 불러올지 말지 결정하는 것\n",
    "                            )\n",
    "taxi_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac34b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_sdf.createOrReplaceTempView(\"mobility_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58ec51a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|pickup_date| trips|\n",
      "+-----------+------+\n",
      "| 2020-03-03|697880|\n",
      "| 2020-03-02|648986|\n",
      "| 2020-03-01|784246|\n",
      "| 2020-03-06|872012|\n",
      "| 2020-03-05|731165|\n",
      "| 2020-03-04|707879|\n",
      "| 2020-03-09|628940|\n",
      "| 2020-03-08|731222|\n",
      "| 2020-03-07|886071|\n",
      "| 2020-03-10|626474|\n",
      "| 2020-03-12|643257|\n",
      "| 2020-03-11|628601|\n",
      "| 2020-03-16|391518|\n",
      "| 2020-03-13|660914|\n",
      "| 2020-03-15|448125|\n",
      "| 2020-03-14|569397|\n",
      "| 2020-03-26|141607|\n",
      "| 2020-03-25|141088|\n",
      "| 2020-03-20|261900|\n",
      "| 2020-03-24|141686|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "\n",
    "select \n",
    "    pickup_date, \n",
    "    count(*) as trips\n",
    "from (  select\n",
    "            split(pickup_datetime, ' ')[0] as pickup_date\n",
    "        from mobility_data )\n",
    "group by pickup_date\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a29db989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['pickup_date], ['pickup_date, 'count(1) AS trips#198]\n",
      "+- 'SubqueryAlias __auto_generated_subquery_name\n",
      "   +- 'Project ['split('pickup_datetime,  )[0] AS pickup_date#197]\n",
      "      +- 'UnresolvedRelation [mobility_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "pickup_date: string, trips: bigint\n",
      "Aggregate [pickup_date#197], [pickup_date#197, count(1) AS trips#198L]\n",
      "+- SubqueryAlias __auto_generated_subquery_name\n",
      "   +- Project [split(pickup_datetime#129,  , -1)[0] AS pickup_date#197]\n",
      "      +- SubqueryAlias mobility_data\n",
      "         +- View (`mobility_data`, [hvfhs_license_num#127,dispatching_base_num#128,pickup_datetime#129,dropoff_datetime#130,PULocationID#131,DOLocationID#132,SR_Flag#133])\n",
      "            +- Relation [hvfhs_license_num#127,dispatching_base_num#128,pickup_datetime#129,dropoff_datetime#130,PULocationID#131,DOLocationID#132,SR_Flag#133] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [pickup_date#197], [pickup_date#197, count(1) AS trips#198L]\n",
      "+- Project [split(pickup_datetime#129,  , -1)[0] AS pickup_date#197]\n",
      "   +- Relation [hvfhs_license_num#127,dispatching_base_num#128,pickup_datetime#129,dropoff_datetime#130,PULocationID#131,DOLocationID#132,SR_Flag#133] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[pickup_date#197], functions=[count(1)], output=[pickup_date#197, trips#198L])\n",
      "   +- Exchange hashpartitioning(pickup_date#197, 200), ENSURE_REQUIREMENTS, [plan_id=164]\n",
      "      +- HashAggregate(keys=[pickup_date#197], functions=[partial_count(1)], output=[pickup_date#197, count#203L])\n",
      "         +- Project [split(pickup_datetime#129,  , -1)[0] AS pickup_date#197]\n",
      "            +- FileScan csv [pickup_datetime#129] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/working/spark-examples/data/fhvhv_tripdata_2020-03.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pickup_datetime:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d1982e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
